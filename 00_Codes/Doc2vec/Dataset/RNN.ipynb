{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1fa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob, os, time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5f75e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[K     |███████████████████████████▊    | 442.8 MB 98.0 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 511.7 MB 644 bytes/s \n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 51.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: packaging in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorflow) (20.9)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 124.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 54.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 57.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 124.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 55.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 85.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 144.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 26.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 156.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 55.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 162.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /home/ichikawa17/anaconda3_1/lib/python3.8/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.1.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.3 importlib-metadata-4.11.4 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69398920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./News_text/09211340緊急事態宣言　全面解除視野に検討　28日に決定へ.csv\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./News_text/*')\n",
    "for i in files:\n",
    "    text = i.split()[-1].split('.csv')[0]\n",
    "    df = pd.read_csv(i)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1b9cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = ''\n",
    "for i in df['コメント']:\n",
    "    text+=i.replace(\"\\n\",\"\")\n",
    "    text += '\\n'\n",
    "text = text.replace(\"\\u3000\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5148f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 69800 characters\n"
     ]
    }
   ],
   "source": [
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838debb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1553 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba23338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれの文字からインデックスへの対応表を作成\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7d43d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '%' :   3,\n",
      "  '&' :   4,\n",
      "  '(' :   5,\n",
      "  ')' :   6,\n",
      "  '+' :   7,\n",
      "  ',' :   8,\n",
      "  '-' :   9,\n",
      "  '.' :  10,\n",
      "  '/' :  11,\n",
      "  '0' :  12,\n",
      "  '1' :  13,\n",
      "  '2' :  14,\n",
      "  '3' :  15,\n",
      "  '4' :  16,\n",
      "  '5' :  17,\n",
      "  '6' :  18,\n",
      "  '7' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20650e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'感染者数の視点からはトレン' ---- characters mapped to int ---- > [ 695  859 1183  782  121 1267  978   87  147  122  192  226  229]\n"
     ]
    }
   ],
   "source": [
    "# テキストの最初の 13 文字がどのように整数に変換されるかを見てみる\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8911c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "感\n",
      "染\n",
      "者\n",
      "数\n",
      "の\n"
     ]
    }
   ],
   "source": [
    "# ひとつの入力としたいシーケンスの文字数としての最大の長さ\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# 訓練用サンプルとターゲットを作る\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b1a0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'感染者数の視点からはトレンドが減少方向ということを含め、十分解除可能だと思われます。病床の方はどうでしょう。入院すべき人が入院できない状況がどうなっているかが気になります。\\nもうオミは無視して決めろよ。'\n",
      "'もう2回打ったし打った方が良いのなら3回目も打つし行動制限なんてもうええ加減にしろよ。\\n毎回遅いねん。２日前に決められても困るしアメリカ行く前に決めろよな…完全に解除なら、通常営業に向けて準備とかあるか'\n",
      "'ら色々困るんですけど一年以上たってまだわからないんですかね？自分らギリギリの生活した事ないからこんな事出来るんでしょうねー。\\n５０才以下のワクチン接種率が５割を超えるまで、解除しない方が良いよ。今の流れ'\n",
      "'は、４波が終わった頃と似ている。その後１か月足らずで、５波が始まったのを、もう忘れてるね。\\nまんえん防止を始めちゃうと，分科会がぐずぐずいうのと，それにまして，知事が調子に乗ってちょっと増えてきたときに'\n",
      "'国費狙いで特に何の思想もなくお金くれ責任は国に預けるよで緊急事態宣言を要請してきます（特に東京大阪はこれやる，これが現状の社会不安の原因です）。離島であることを考慮し沖縄には対策慎重になるとしても他は解'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81c899b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a647949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '感染者数の視点からはトレンドが減少方向ということを含め、十分解除可能だと思われます。病床の方はどうでしょう。入院すべき人が入院できない状況がどうなっているかが気になります。\\nもうオミは無視して決めろよ'\n",
      "Target data: '染者数の視点からはトレンドが減少方向ということを含め、十分解除可能だと思われます。病床の方はどうでしょう。入院すべき人が入院できない状況がどうなっているかが気になります。\\nもうオミは無視して決めろよ。'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b605fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 695 ('感')\n",
      "  expected output: 859 ('染')\n",
      "Step    1\n",
      "  input: 859 ('染')\n",
      "  expected output: 1183 ('者')\n",
      "Step    2\n",
      "  input: 1183 ('者')\n",
      "  expected output: 782 ('数')\n",
      "Step    3\n",
      "  input: 782 ('数')\n",
      "  expected output: 121 ('の')\n",
      "Step    4\n",
      "  input: 121 ('の')\n",
      "  expected output: 1267 ('視')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57d807f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# バッチサイズ\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# データセットをシャッフルするためのバッファサイズ\n",
    "# （TF data は可能性として無限長のシーケンスでも使えるように設計されています。\n",
    "# このため、シーケンス全体をメモリ内でシャッフルしようとはしません。\n",
    "# その代わりに、要素をシャッフルするためのバッファを保持しています）\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2316626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字数で表されるボキャブラリーの長さ\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 埋め込みベクトルの次元\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN ユニットの数\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f254a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential(\n",
    "        [tf.keras.layers.Embedding(vocab_size,\n",
    "                                   embedding_dim,\n",
    "                                   batch_input_shape=[batch_size, None]),\n",
    "         tf.keras.layers.GRU(rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             stateful=True,\n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "         tf.keras.layers.Dense(vocab_size)\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75332156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5bd3096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 1553) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d4f6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           397568    \n",
      "                                                                 \n",
      " gru (GRU)                   (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 1553)          1591825   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,927,697\n",
      "Trainable params: 5,927,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f156536",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b30f22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 365,  550, 1236,   97, 1195,  242,  425,  946,  318, 1123,  432,\n",
       "         44,   60,  413,  164,  941,  533, 1303, 1042, 1245, 1042,  261,\n",
       "       1101,  560,  363,   83, 1324, 1150,  497, 1200,  816, 1109,  846,\n",
       "       1033,  525,  899, 1079,  462, 1023,  973,  460,  240, 1495,  347,\n",
       "       1485,  714, 1431,  242,   17,  140,  587,  926, 1181,  348,  558,\n",
       "       1364,   85,  872,  232,  965,  808,  739, 1497, 1498,  404,  479,\n",
       "       1157, 1454,  472,  381, 1138, 1360,   82,  711, 1442,  791,  171,\n",
       "       1224,  623, 1103,  948, 1368,  617, 1388, 1458, 1205,  975, 1407,\n",
       "       1043, 1047,  218,  259, 1504,  315,  405, 1134,  514,  132, 1420,\n",
       "        399])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4222ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '分可能なはずだ。そこまで持っていく前の活動再開は、いい結果に向かわない。\\nやっとだな\\nやっとだな\\n今までと同じ様な繰り返しにならないといいけど。\\n全面解除？？段階的じゃなくて？\\n地味ん新総裁誕生記念解'\n",
      "\n",
      "Next Char Predictions: \n",
      " '切完蔑さ肺不去深倣糸取d…卓オ海始請発衆発事立宰出ぇ貰綱堂脅晩等村疲奈殖禍哀町濃和上駄入飛所閥不5も屋況老全室近ぉ森・演映括駆騒包回線雇喫剥終辺う戯陰新ケ花底端添述幹運電腐火酬白盆ャ予高倒化素夕ぺ鎌務'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edce90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 1553)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       7.347842\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "458ff296",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42da0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# チェックポイントが保存されるディレクトリ\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# チェックポイントファイルの名称\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd33cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82700d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10/10 [==============================] - 7s 563ms/step - loss: 6.4631\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 6s 556ms/step - loss: 5.4794\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 5.3575\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 5.2106\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 4.9864\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 4.7310\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 4.4641\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 4.2875\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 6s 560ms/step - loss: 4.1749\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 4.0091\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 6s 557ms/step - loss: 3.8592\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 3.7291\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 6s 552ms/step - loss: 3.6148\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 3.5149\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 3.4200\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 6s 556ms/step - loss: 3.3291\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 6s 557ms/step - loss: 3.2500\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 6s 561ms/step - loss: 3.1833\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 6s 557ms/step - loss: 3.1069\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 3.0292\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 2.9620\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 2.9005\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 2.8288\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 6s 557ms/step - loss: 2.7645\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 2.6866\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 2.6016\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 6s 561ms/step - loss: 2.5270\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 2.4579\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 6s 558ms/step - loss: 2.3907\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 6s 555ms/step - loss: 2.3174\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dde78652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_30'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b50de10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            397568    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 1553)           1591825   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,927,697\n",
      "Trainable params: 5,927,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "143c865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # 評価ステップ（学習済みモデルを使ったテキスト生成）\n",
    "\n",
    "  # 生成する文字数\n",
    "    num_generate = 100\n",
    "\n",
    "  # 開始文字列を数値に変換（ベクトル化）\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "  # 結果を保存する空文字列\n",
    "    text_generated = []\n",
    "\n",
    "  # 低い temperature　は、より予測しやすいテキストをもたらし\n",
    "  # 高い temperature は、より意外なテキストをもたらす\n",
    "  # 実験により最適な設定を見つけること\n",
    "    temperature = 1.0\n",
    "\n",
    "  # ここではバッチサイズ　== 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      # バッチの次元を削除\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # カテゴリー分布をつかってモデルから返された文字を予測 \n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # 過去の隠れ状態とともに予測された文字をモデルへのつぎの入力として渡す\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1018efae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "感染者が減少してから想定すれば良いでしょう\n",
      "解除したら、急・・宣言」できついたらどうコロナ禍にそんじゃないとわるのがいいんじゃな。\n",
      "治っ的に全て良いと思ってるだきたないから思い\n",
      "仕込み直状態陽性者数の確認に\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,\n",
    "                    start_string=u\"感染者\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7373896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "587a8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                target, predictions, from_logits=True))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dd21f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 5.6404\n",
      "Time taken for 1 epoch 6.405139207839966 sec\n",
      "\n",
      "Epoch 2 Loss 5.3975\n",
      "Time taken for 1 epoch 5.497836112976074 sec\n",
      "\n",
      "Epoch 3 Loss 5.3376\n",
      "Time taken for 1 epoch 5.4719154834747314 sec\n",
      "\n",
      "Epoch 4 Loss 5.1549\n",
      "Time taken for 1 epoch 5.480853080749512 sec\n",
      "\n",
      "Epoch 5 Loss 4.9070\n",
      "Time taken for 1 epoch 5.504909992218018 sec\n",
      "\n",
      "Epoch 6 Loss 4.6821\n",
      "Time taken for 1 epoch 5.474076986312866 sec\n",
      "\n",
      "Epoch 7 Loss 4.4087\n",
      "Time taken for 1 epoch 5.490312814712524 sec\n",
      "\n",
      "Epoch 8 Loss 4.1618\n",
      "Time taken for 1 epoch 5.454525947570801 sec\n",
      "\n",
      "Epoch 9 Loss 4.0911\n",
      "Time taken for 1 epoch 5.534043788909912 sec\n",
      "\n",
      "Epoch 10 Loss 3.8981\n",
      "Time taken for 1 epoch 5.569545745849609 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 訓練ステップ\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "  # 各エポックの最初に、隠れ状態を初期化する\n",
    "  # 最初は隠れ状態は None\n",
    "    hidden = model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "        template = 'Epoch {} Batch {} Loss {}'\n",
    "        print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # 5エポックごとにモデル（のチェックポイント）を保存する\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0abd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
