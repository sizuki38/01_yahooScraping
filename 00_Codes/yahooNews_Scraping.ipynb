{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import time, datetime, MeCab, collections, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chromeDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-a990e741f5d2>:4: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(\"/usr/local/bin/chromedriver\",  chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "#chromeをheadlessで起動\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(\"/usr/local/bin/chromedriver\",  chrome_options=options)\n",
    "#driver = webdriver.Chrome(\"/usr/local/bin/chromedriver\")\n",
    "top_page = \"https://news.yahoo.co.jp/\"\n",
    "dt_now = datetime.datetime.now()\n",
    "driver.get(top_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニュースリストの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中継 西村担当相と尾身会長が臨時会見\n",
      "「完全離脱」カウントダウン　貿易交渉、なお溝　英EU\n",
      "トヨタ自動車元町工場の火事は鎮火　開発棟内の車両から出火か　生産ラインに影響なし\n",
      "三陽商会が正社員の早期退職募集、人数定めず　保養所も売却\n",
      "東急線20年ぶり新駅は「新綱島駅」。東急新横浜線'22年度下期開業\n",
      "西武、松坂と来季1000万円減の2000万円で契約締結…「少しでも恩返しできるように」\n",
      "宮崎大輔、離婚成立を認めた！ “暴行逮捕”騒動の13歳下恋人と「今も一緒に住んでます」\n",
      "林家こん平さん死去　７７歳「笑点」で人気「１、２、３、チャラ～ン」\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['news_title','comment_url'])\n",
    "\n",
    "#topicの選択\n",
    "select_M_Topic = driver.find_element_by_link_text(\"トップ\").click()\n",
    "time.sleep(0.1)\n",
    "select_S_Topic = driver.find_element_by_link_text(\"主要\").click()\n",
    "time.sleep(0.1)\n",
    "\n",
    "#ニュース記事への移動\n",
    "topPage = driver.current_url\n",
    "#news[0].click()\n",
    "for a in range(8):\n",
    "#     print(a+1)\n",
    "    section = driver.find_element_by_tag_name(\"section\")\n",
    "    news_list = section.find_elements_by_tag_name(\"li\")\n",
    "    mv_news = news_list[a].click()\n",
    "    time.sleep(0.1)\n",
    "    news_main = driver.find_element_by_tag_name(\"article\")\n",
    "    news_title = news_main.find_elements_by_tag_name(\"p\")\n",
    "    print(news_title[2].text)\n",
    "\n",
    "    #コメントページへの移動\n",
    "    try:\n",
    "        news_comment_url = driver.find_element_by_class_name(\"news-comment-plugin\").get_attribute(\"data-full-page-url\")\n",
    "#         print(news_comment_url)\n",
    "        df.loc[a] = [news_title[2].text,news_comment_url]    \n",
    "    except:\n",
    "#         print('This news page have no comment.')\n",
    "        df.loc[a] = [news_title[2].text,'This news page have no comment.']    \n",
    "    finally:\n",
    "        driver.get(topPage) \n",
    "        #time.sleep(1)\n",
    "\n",
    "dt = dt_now.strftime('%Y%m%d')\n",
    "df.to_csv('./NewsList/'+str(dt)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コメント情報の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中継 西村担当相と尾身会長が臨時会見\n",
      "No_Comments.\n",
      "「完全離脱」カウントダウン　貿易交渉、なお溝　英EU\n",
      "Have a Comments.\n",
      "「完全離脱」カウントダウン　貿易交渉、なお溝　英EU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-b1db154b7553>:30: DeprecationWarning: use driver.switch_to.frame instead\n",
      "  driver.switch_to_frame(iframe)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b1db154b7553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pagenation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_elements_by_class_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rapidnofollow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mlast_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to_default_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dt = dt_now.strftime('%Y%m%d')\n",
    "df = pd.read_csv('./NewsList/'+str(dt)+'.csv')\n",
    "for i in range(df.shape[0]):\n",
    "    title = df['news_title'][i]\n",
    "    print(title)\n",
    "    try:    \n",
    "        url = df['comment_url'][i]\n",
    "        driver.get(url)\n",
    "    except:\n",
    "        print('No_Comments.')\n",
    "        continue\n",
    "    else:\n",
    "        print('Have a Comments.')\n",
    "        break\n",
    "print(title)\n",
    "\n",
    "#コメント情報の取得\n",
    "\n",
    "comment_boxes = []\n",
    "\n",
    "#df = pd.DataFrame(columns=['number','日付','ユーザー名','コメント','good','bad'])\n",
    "df = pd.DataFrame(columns=['number','日付','ユーザー名','コメント'])\n",
    "start = 1\n",
    "end = 200\n",
    "base_url = driver.current_url\n",
    "page_url = '?page='\n",
    "order_url = '&order=older'\n",
    "\n",
    "iframe = driver.find_element_by_class_name(\"news-comment-plguin-iframe\")\n",
    "driver.switch_to_frame(iframe)\n",
    "end = driver.find_element_by_class_name(\"pagenation\")\n",
    "pages = end.find_elements_by_class_name(\"rapidnofollow\")\n",
    "last_page = pages[7].text\n",
    "driver.switch_to_default_content()\n",
    "print(last_page)\n",
    "for page in range(start,int(last_page)):\n",
    "    \n",
    "    driver.get(base_url+page_url+str(page)+order_url) \n",
    "    print(driver.current_url)\n",
    "#    time.sleep(1)\n",
    "    iframe = driver.find_element_by_class_name(\"news-comment-plguin-iframe\")\n",
    "    driver.switch_to_frame(iframe)\n",
    "    \n",
    "    comment_boxes = driver.find_elements_by_class_name(\"root\")\n",
    "    page_comment =0\n",
    "    \n",
    "    for comment_box in comment_boxes:\n",
    "        \n",
    "\n",
    "        \n",
    "        page_comment +=1\n",
    "        comment_number = str(page)+'-'+str(page_comment)\n",
    "        \n",
    "        #ユーザー名取得\n",
    "        elem_name = comment_box.find_element_by_class_name(\"rapidnofollow\")\n",
    "        name = elem_name.text\n",
    "        #names.append(name)\n",
    "        \n",
    "        #日付取得\n",
    "        elem_date = comment_box.find_element_by_class_name(\"date\")\n",
    "        date = elem_date.text\n",
    "        #dates.append(date) \n",
    "        \n",
    "        #コメント取得\n",
    "        try:\n",
    "            elem_comment = comment_box.find_element_by_class_name(\"cmtBody\")\n",
    "        except:\n",
    "            elem_comment = comment_box.find_element_by_class_name(\"yjxComment\")\n",
    "\n",
    "        comment = elem_comment.text\n",
    "        comment.strip(\"\\n\")\n",
    "        #comments.append(comment)\n",
    "        \n",
    "        #good数取得\n",
    "        try:\n",
    "            agree_box = comment_box.find_element_by_class_name(\"good\")\n",
    "            elem_agree = agree_box.find_element_by_class_name(\"userNum\")\n",
    "            agree = elem_agree.text\n",
    "        except:\n",
    "            pass\n",
    "        #agrees.append(agree)\n",
    "\n",
    "        #bad数取得\n",
    "        try:\n",
    "            disagree_box = comment_box.find_element_by_class_name(\"bad\")\n",
    "            elem_disagree = disagree_box.find_element_by_class_name(\"userNum\")\n",
    "            disagree = elem_disagree.text\n",
    "        except:\n",
    "            pass\n",
    "        #disagrees.append(disagree)\n",
    "        #df.loc[(page-1)*10+page_comment] = [comment_number,date, name, comment, agree, disagree]\n",
    "        df.loc[(page-1)*10+page_comment] = [comment_number,date, name, comment]\n",
    "\n",
    "\n",
    "df.to_csv('./NewsComment/'+title+\".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yahoo_news_comment.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-3703b79ec0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yahoo_news_comment.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwakati\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-Owakati\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchasen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-Ochasen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcomment_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'コメント'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yahoo_news_comment.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('yahoo_news_comment.csv')\n",
    "\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "chasen = MeCab.Tagger(\"-Ochasen\")\n",
    "comment_list = df['コメント']\n",
    "for a in range(0,df.shape[0]):\n",
    "    print(a)\n",
    "#    all_comment += str(comment_list[a])\n",
    "#print(all_comment)\n",
    "# words = wakati.parse(all_comment).split()\n",
    "# word_count = collections.Counter(words)\n",
    "# print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'「完全離脱」カウントダウン\\u3000貿易交渉、なお溝\\u3000英EU'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./NewsList/'+str(dt)+'.csv')\n",
    "df.shape[0]\n",
    "df['news_title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
